[
  {
    "id": "Q3-AI-V101-01",
    "chapter": "第3章",
    "level": "K4",
    "category": "AI/ML",
    "style": "シナリオ",
    "type": "単一選択",
    "question": "【分析】AIモデルの公平性（Fairness）テスト。\n人材採用支援AIを導入したが、特定の性別や年齢層に対して不利な判定が出ている疑いがある。\nこの「バイアス」の原因を特定し、改善するためにテスターが最初に行うべき分析アクションはどれか。",
    "options": [
      "AIの判断ロジックはブラックボックスであり解析不可能であるため、テストデータではなくアルゴリズム自体のパラメータをランダムに調整して、出力結果のバランスが良くなる設定値を探索する",
      "トレーニングデータセット（学習データ）の構成比率を分析し、特定の属性グループに対するデータ不足や、過去の採用履歴に含まれる人間由来のバイアスが学習データに反映されていないかを確認する",
      "AIが出した不採用通知に対して、人間が一件ずつ再審査を行い、AIの間違いを全て手動で修正してから結果を通知する運用フローに変更する",
      "バイアスは統計的な誤差に過ぎないため、テストデータのサンプル数を単純に10倍に増やし、大数の法則によって誤差が収束することを期待して再学習させる"
    ],
    "answer": [
      "トレーニングデータセット（学習データ）の構成比率を分析し、特定の属性グループに対するデータ不足や、過去の採用履歴に含まれる人間由来のバイアスが学習データに反映されていないかを確認する"
    ],
    "explanation": "【解説】\nAIのバイアスの大半は「データ」に起因します。アルゴリズムをいじる前に、学習データ自体が公平か（Data Bias）を疑うのが鉄則です。",
    "tags": [
      "第3章",
      "AI/ML",
      "シナリオ",
      "K4"
    ]
  }
]